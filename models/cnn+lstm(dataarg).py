# -*- coding: utf-8 -*-
"""cnn+lstm(dataarg).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HjYyLgPL2KQoj3amNqTuXFXQm2c2jTyK
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score
import matplotlib.pyplot as plt
import random

from google.colab import drive
drive.mount('/content/drive')

train_df = pd.read_csv("/content/drive/MyDrive/train.csv")
test_df  = pd.read_csv("/content/drive/MyDrive/test.csv")

# Target minority classes
minority_classes = [2, 3]  # 2=Anxiety, 3=Stress

def simple_paraphrase(text):
    # Very basic rewording by replacing some words
    text = text.replace("I feel", "I'm feeling")
    text = text.replace("anxious", "nervous")
    text = text.replace("stress", "pressure")
    return text

augmented_texts = []
augmented_labels = []

for cls in minority_classes:
    subset = train_df[train_df['label'] == cls]
    for text in subset['statement']:
        # create 1-2 paraphrased versions
        for _ in range(2):
            augmented_texts.append(simple_paraphrase(text))
            augmented_labels.append(cls)

print(f"Added {len(augmented_texts)} augmented minority class examples")

# Append to original training data
X_train = list(train_df['statement']) + augmented_texts
y_train = list(train_df['label']) + augmented_labels

X_test = list(test_df['statement'])
y_test = list(test_df['label'])

import re

# Simple tokenizer
def tokenize(text):
    text = text.lower()
    tokens = re.findall(r'\b\w+\b', text)
    return tokens

# Build vocab
vocab = {"<pad>":0, "<unk>":1}
idx = 2
for text in X_train:
    for token in tokenize(text):
        if token not in vocab:
            vocab[token] = idx
            idx += 1

vocab_size = len(vocab)
print(f"Vocab size: {vocab_size}")

# Encode function
def encode(texts, max_len=200):
    encoded = []
    for text in texts:
        tokens = tokenize(text)
        token_ids = [vocab.get(t, vocab["<unk>"]) for t in tokens][:max_len]
        token_ids += [vocab["<pad>"]] * (max_len - len(token_ids))
        encoded.append(token_ids)
    return torch.tensor(encoded, dtype=torch.long)

X_train_tok = encode(X_train)
X_test_tok  = encode(X_test)
y_train_t = torch.tensor(y_train, dtype=torch.long)
y_test_t  = torch.tensor(y_test, dtype=torch.long)

train_ds = TensorDataset(X_train_tok, y_train_t)
test_ds  = TensorDataset(X_test_tok, y_test_t)

train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
test_loader  = DataLoader(test_ds, batch_size=32)

class_counts = np.bincount(y_train)
weights = 1.0 / class_counts
weights = weights / weights.sum()
class_weights = torch.tensor(weights, dtype=torch.float32)

class CNN_LSTM(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_classes):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.conv = nn.Conv1d(embed_dim, 128, kernel_size=3, padding=1)
        self.lstm = nn.LSTM(128, 128, batch_first=True)
        self.fc = nn.Linear(128, num_classes)

    def forward(self, x):
        x = self.embedding(x)
        x = x.permute(0, 2, 1)  # (batch, channels, seq_len)
        x = torch.relu(self.conv(x))
        x = x.permute(0, 2, 1)  # (batch, seq_len, channels)
        _, (h, _) = self.lstm(x)
        return self.fc(h[-1])

model = CNN_LSTM(len(vocab), 300, 4)

criterion = nn.CrossEntropyLoss(weight=class_weights)
optimizer = optim.Adam(model.parameters(), lr=1e-3)

epochs = 10

for epoch in range(epochs):
    model.train()
    total_loss = 0
    for xb, yb in train_loader:
        optimizer.zero_grad()
        loss = criterion(model(xb), yb)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}/{epochs} Loss: {total_loss:.4f}")

model.eval()
preds = []

with torch.no_grad():
    for xb, _ in test_loader:
        preds.extend(torch.argmax(model(xb), dim=1).cpu().numpy())

print(classification_report(
    y_test,
    preds,
    target_names=["Normal", "Depression", "Anxiety", "Stress"]
))

macro_f1 = f1_score(y_test, preds, average="macro")
print(f"Macro F1: {macro_f1:.4f}")

cm = confusion_matrix(y_test, preds)
disp = ConfusionMatrixDisplay(cm, display_labels=["Normal", "Depression", "Anxiety", "Stress"])
disp.plot(cmap="Blues")
plt.title("CNN + LSTM (Augmented Minority Classes)")
plt.show()