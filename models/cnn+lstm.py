# -*- coding: utf-8 -*-
"""hybrid.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lVFlYcwosIWJXFXs3h1rkue_FfZm3brY
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

train_df = pd.read_csv("/content/drive/MyDrive/train.csv")
test_df  = pd.read_csv("/content/drive/MyDrive/test.csv")

X_train = train_df["statement"].tolist()
y_train = train_df["label"].values

X_test = test_df["statement"].tolist()
y_test = test_df["label"].values

import re

# Simple tokenizer: lowercase + split on non-word chars
def tokenize(text):
    text = text.lower()
    tokens = re.findall(r'\b\w+\b', text)
    return tokens

# Build vocab from training data
vocab = {"<pad>":0, "<unk>":1}
idx = 2
for text in X_train:
    for token in tokenize(text):
        if token not in vocab:
            vocab[token] = idx
            idx += 1

vocab_size = len(vocab)
print(f"Vocab size: {vocab_size}")

# Encode function: convert text â†’ list of token indices, pad to max_len
def encode(texts, max_len=200):
    encoded = []
    for text in texts:
        tokens = tokenize(text)
        token_ids = [vocab.get(t, vocab["<unk>"]) for t in tokens][:max_len]
        # Pad
        token_ids += [vocab["<pad>"]] * (max_len - len(token_ids))
        encoded.append(token_ids)
    return torch.tensor(encoded, dtype=torch.long)

# Encode train & test
X_train_tok = encode(X_train)
X_test_tok  = encode(X_test)

y_train_t = torch.tensor(y_train, dtype=torch.long)
y_test_t  = torch.tensor(y_test, dtype=torch.long)

train_ds = TensorDataset(X_train_tok, y_train_t)
test_ds  = TensorDataset(X_test_tok, y_test_t)

train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
test_loader  = DataLoader(test_ds, batch_size=32)

class_counts = np.bincount(y_train)
weights = 1.0 / class_counts
weights = weights / weights.sum()
class_weights = torch.tensor(weights, dtype=torch.float32)

class CNN_LSTM(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_classes):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.conv = nn.Conv1d(embed_dim, 128, kernel_size=3, padding=1)
        self.lstm = nn.LSTM(128, 128, batch_first=True)
        self.fc = nn.Linear(128, num_classes)

    def forward(self, x):
        x = self.embedding(x)
        x = x.permute(0, 2, 1)
        x = torch.relu(self.conv(x))
        x = x.permute(0, 2, 1)
        _, (h, _) = self.lstm(x)
        return self.fc(h[-1])

model = CNN_LSTM(len(vocab), 300, 4)

criterion = nn.CrossEntropyLoss(weight=class_weights)
optimizer = optim.Adam(model.parameters(), lr=1e-3)

epochs = 10

for epoch in range(epochs):
    model.train()
    total_loss = 0
    for xb, yb in train_loader:
        optimizer.zero_grad()
        loss = criterion(model(xb), yb)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}/{epochs} Loss: {total_loss:.4f}")

model.eval()
preds = []

with torch.no_grad():
    for xb, _ in test_loader:
        preds.extend(torch.argmax(model(xb), dim=1).cpu().numpy())

print(classification_report(
    y_test,
    preds,
    target_names=["Normal", "Depression", "Anxiety", "Stress"]
))

f1_score(y_test, preds, average="macro")

cm = confusion_matrix(y_test, preds)
disp = ConfusionMatrixDisplay(cm, display_labels=["Normal", "Depression", "Anxiety", "Stress"])
disp.plot(cmap="Blues")
plt.title("CNN + LSTM (Class Weighted)")
plt.show()