# -*- coding: utf-8 -*-
"""mlp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TBDAAiy6p7Y0HLNBVFCPLZijhMix6wDL
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

train_df = pd.read_csv("/content/drive/MyDrive/train.csv")
test_df  = pd.read_csv("/content/drive/MyDrive/test.csv")

X_train = train_df["statement"].tolist()
y_train = train_df["label"].values

X_test = test_df["statement"].tolist()
y_test = test_df["label"].values

!pip install -q sentence-transformers
from sentence_transformers import SentenceTransformer

sbert = SentenceTransformer("all-MiniLM-L6-v2")

X_train_emb = sbert.encode(X_train, batch_size=32, show_progress_bar=True)
X_test_emb  = sbert.encode(X_test, batch_size=32, show_progress_bar=True)

X_train_t = torch.tensor(X_train_emb, dtype=torch.float32)
y_train_t = torch.tensor(y_train, dtype=torch.long)

X_test_t = torch.tensor(X_test_emb, dtype=torch.float32)
y_test_t = torch.tensor(y_test, dtype=torch.long)

train_ds = TensorDataset(X_train_t, y_train_t)
test_ds  = TensorDataset(X_test_t, y_test_t)

train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
test_loader  = DataLoader(test_ds, batch_size=32, shuffle=False)

class_counts = np.bincount(y_train)
class_weights = 1.0 / class_counts
class_weights = class_weights / class_weights.sum()

class_weights = torch.tensor(class_weights, dtype=torch.float32)

class MLP(nn.Module):
    def __init__(self, input_dim, num_classes):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        return self.net(x)

model = MLP(input_dim=X_train_t.shape[1], num_classes=4)

criterion = nn.CrossEntropyLoss(weight=class_weights)
optimizer = optim.Adam(model.parameters(), lr=1e-3)

epochs = 10

for epoch in range(epochs):
    model.train()
    total_loss = 0

    for xb, yb in train_loader:
        optimizer.zero_grad()
        logits = model(xb)
        loss = criterion(logits, yb)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}")

model.eval()
all_preds = []

with torch.no_grad():
    for xb, _ in test_loader:
        logits = model(xb)
        preds = torch.argmax(logits, dim=1)
        all_preds.extend(preds.cpu().numpy())

print(classification_report(
    y_test,
    all_preds,
    target_names=["Normal", "Depression", "Anxiety", "Stress"]
))

f1 = f1_score(y_test, all_preds, average="macro")
f1

cm = confusion_matrix(y_test, all_preds)

disp = ConfusionMatrixDisplay(
    confusion_matrix=cm,
    display_labels=["Normal", "Depression", "Anxiety", "Stress"]
)

disp.plot(cmap="Blues")
plt.title("SBERT + MLP (Class-Weighted)")
plt.show()

report = classification_report(
    y_test,
    all_preds,
    target_names=["Normal", "Depression", "Anxiety", "Stress"],
    output_dict=True
)

f1_scores = [report[c]["f1-score"] for c in ["Normal", "Depression", "Anxiety", "Stress"]]

plt.bar(["Normal", "Depression", "Anxiety", "Stress"], f1_scores)
plt.ylabel("F1-score")
plt.title("Per-Class F1 â€” SBERT + MLP")
plt.show()