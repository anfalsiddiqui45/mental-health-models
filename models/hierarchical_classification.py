# -*- coding: utf-8 -*-
"""hierarchical-classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wm_QCzTLwNxD0qrqsOQt1Hit0thf1uWJ
"""

!pip install gensim nltk scikit-learn pandas numpy

import numpy as np
import pandas as pd
import nltk
import re

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, f1_score

from gensim.models import FastText
from nltk.tokenize import word_tokenize

nltk.download('punkt')

from google.colab import drive
drive.mount('/content/drive')

train_df = pd.read_csv("/content/drive/MyDrive/train.csv")
test_df  = pd.read_csv("/content/drive/MyDrive/test.csv")

train_df.head(), train_df['label'].value_counts()

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

import re
import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt')

def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"[^a-z\s]", "", text)
    return text

for df in [train_df, test_df]:
    df['clean_text'] = df['statement'].apply(clean_text)
    df['tokens'] = df['clean_text'].apply(word_tokenize)

depression_words = {
    "hopeless","worthless","empty","tired","numb","alone",
    "sad","depressed","guilty","helpless"
}

stress_words = {
    "pressure","deadline","overwhelmed","workload",
    "responsibility","burden","stress"
}

anxiety_words = {
    "worried","panic","fear","anxious",
    "nervous","restless","uneasy"
}

def lexicon_features(tokens):
    return [
        sum(w in depression_words for w in tokens),
        sum(w in anxiety_words for w in tokens),
        sum(w in stress_words for w in tokens),
        len(tokens)
    ]

train_df['lex'] = train_df['tokens'].apply(lexicon_features)
test_df['lex']  = test_df['tokens'].apply(lexicon_features)

from gensim.models import FastText
import numpy as np

sentences = train_df['tokens'].tolist()

ft_model = FastText(
    sentences=sentences,
    vector_size=100,
    window=5,
    min_count=2,
    workers=4,
    epochs=10
)

def sentence_embedding(tokens):
    vecs = [ft_model.wv[w] for w in tokens if w in ft_model.wv]
    if len(vecs) == 0:
        return np.zeros(100)
    return np.mean(vecs, axis=0)

train_df['embed'] = train_df['tokens'].apply(sentence_embedding)
test_df['embed']  = test_df['tokens'].apply(sentence_embedding)

X_train = np.hstack([
    np.vstack(train_df['embed']),
    np.vstack(train_df['lex'])
])

X_test = np.hstack([
    np.vstack(test_df['embed']),
    np.vstack(test_df['lex'])
])

y_train = train_df['label']
y_test  = test_df['label']

"""Normal is forced out first, so the model stops confusing Normal and Depression due to surface overlap."""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

y_train_s1 = y_train.apply(lambda x: 0 if x == 0 else 1)
y_test_s1  = y_test.apply(lambda x: 0 if x == 0 else 1)

stage1 = LogisticRegression(
    class_weight='balanced',
    max_iter=1000
)

stage1.fit(X_train, y_train_s1)

print("STAGE 1 RESULTS")
print(classification_report(y_test_s1, stage1.predict(X_test)))

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Stage 1: Normal vs Distress
y_pred_s1 = stage1.predict(X_test)

cm_s1 = confusion_matrix(y_test_s1, y_pred_s1)
plt.figure(figsize=(5,4))
sns.heatmap(cm_s1, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Normal','Distress'],
            yticklabels=['Normal','Distress'])
plt.title("Confusion Matrix - Stage 1")
plt.ylabel("Actual")
plt.xlabel("Predicted")
plt.show()

from sklearn.svm import SVC
from sklearn.metrics import f1_score

train_mask = y_train != 0
test_mask  = y_test != 0

X_train_s2 = X_train[train_mask]
y_train_s2 = y_train[train_mask]

X_test_s2 = X_test[test_mask]
y_test_s2 = y_test[test_mask]

stage2 = SVC(
    kernel='rbf',
    class_weight='balanced'
)

stage2.fit(X_train_s2, y_train_s2)

y_pred = stage2.predict(X_test_s2)

print("STAGE 2 RESULTS")
print(classification_report(y_test_s2, y_pred))
print("Macro F1:", f1_score(y_test_s2, y_pred, average='macro'))

# Stage 2: Depression vs Anxiety vs Stress (only Distress)
y_pred_s2 = stage2.predict(X_test_s2)

cm_s2 = confusion_matrix(y_test_s2, y_pred_s2)
plt.figure(figsize=(6,5))
sns.heatmap(cm_s2, annot=True, fmt='d', cmap='Greens',
            xticklabels=['Depression','Anxiety','Stress'],
            yticklabels=['Depression','Anxiety','Stress'])
plt.title("Confusion Matrix - Stage 2")
plt.ylabel("Actual")
plt.xlabel("Predicted")
plt.show()

import numpy as np
from sklearn.preprocessing import LabelEncoder

# Only Distress classes (1,2,3)
train_mask = y_train != 0
test_mask  = y_test != 0

X_train_s2_embed = np.vstack(train_df['embed'][train_mask])
X_test_s2_embed  = np.vstack(test_df['embed'][test_mask])

X_train_s2_lex = np.vstack(train_df['lex'][train_mask])
X_test_s2_lex  = np.vstack(test_df['lex'][test_mask])

# Combine embeddings + lexicon features
X_train_s2 = np.hstack([X_train_s2_embed, X_train_s2_lex])
X_test_s2  = np.hstack([X_test_s2_embed, X_test_s2_lex])

y_train_s2 = y_train[train_mask].values
y_test_s2  = y_test[test_mask].values

# Encode labels to 0,1,2 for neural network
le = LabelEncoder()
y_train_s2_enc = le.fit_transform(y_train_s2)
y_test_s2_enc  = le.transform(y_test_s2)

# Convert to categorical for PyTorch later
import torch
y_train_s2_tensor = torch.tensor(y_train_s2_enc, dtype=torch.long)
y_test_s2_tensor  = torch.tensor(y_test_s2_enc, dtype=torch.long)

import torch
from torch.utils.data import Dataset, DataLoader

class Stage2Dataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = y

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

train_dataset = Stage2Dataset(X_train_s2, y_train_s2_tensor)
test_dataset  = Stage2Dataset(X_test_s2, y_test_s2_tensor)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader  = DataLoader(test_dataset, batch_size=32)

import torch.nn as nn
import torch.nn.functional as F

class CNN_LSTM_Classifier(nn.Module):
    def __init__(self, input_dim, num_classes):
        super(CNN_LSTM_Classifier, self).__init__()
        self.input_dim = input_dim

        # CNN part
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)
        self.pool  = nn.AdaptiveMaxPool1d(1)

        # LSTM part
        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=128, batch_first=True, bidirectional=True)

        # Fully connected
        self.fc1 = nn.Linear(128*2 + 128, 64)
        self.fc2 = nn.Linear(64, num_classes)

    def forward(self, x):
        # x: [batch, features]
        x_cnn = x.unsqueeze(1)  # add channel dim
        x_cnn = F.relu(self.conv1(x_cnn))
        x_cnn = F.relu(self.conv2(x_cnn))
        x_cnn = self.pool(x_cnn).squeeze(-1)

        # LSTM
        x_lstm = x.unsqueeze(1)  # seq_len=1 for sentence embeddings
        _, (h_n, _) = self.lstm(x_lstm)
        x_lstm = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)  # bidirectional

        # Fusion
        x = torch.cat([x_cnn, x_lstm], dim=1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

num_classes = len(np.unique(y_train_s2_enc))
input_dim = X_train_s2.shape[1]

# Compute class weights
from sklearn.utils.class_weight import compute_class_weight
class_weights = compute_class_weight('balanced', classes=np.unique(y_train_s2_enc), y=y_train_s2_enc)
class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)

model = CNN_LSTM_Classifier(input_dim=input_dim, num_classes=num_classes).to(device)
criterion = nn.CrossEntropyLoss(weight=class_weights)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

epochs = 10

for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    for X_batch, y_batch in train_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * X_batch.size(0)
    epoch_loss = running_loss / len(train_loader.dataset)
    print(f"Epoch {epoch+1}/{epochs} Loss: {epoch_loss:.4f}")

from sklearn.metrics import classification_report, f1_score

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for X_batch, y_batch in test_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        outputs = model(X_batch)
        preds = torch.argmax(outputs, dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(y_batch.cpu().numpy())

print("STAGE 2 HYBRID CNN+LSTM RESULTS")
print(classification_report(all_labels, all_preds, target_names=['Depression','Anxiety','Stress']))
print("Macro F1:", f1_score(all_labels, all_preds, average='macro'))

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Labels for Stage 2
labels_s2 = ['Depression', 'Anxiety', 'Stress']

# Compute confusion matrix
cm_s2 = confusion_matrix(all_labels, all_preds)

# Plot
plt.figure(figsize=(6,5))
sns.heatmap(cm_s2, annot=True, fmt='d', cmap='Blues',
            xticklabels=labels_s2, yticklabels=labels_s2)
plt.title("Stage 2: Hybrid CNN+LSTM Confusion Matrix")
plt.ylabel("Actual")
plt.xlabel("Predicted")
plt.show()